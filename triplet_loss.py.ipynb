{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef8c9029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def normalize(x, axis=-1):\n",
    "    \"\"\"Normalizing to unit length along the specified dimension.\n",
    "    Args:\n",
    "      x: pytorch Variable\n",
    "    Returns:\n",
    "      x: pytorch Variable, same shape as input\n",
    "    \"\"\"\n",
    "    x = 1. * x / (torch.norm(x, 2, axis, keepdim=True).expand_as(x) + 1e-6)\n",
    "    return x\n",
    "\n",
    "\n",
    "def euclidean_dist(x, y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      x: pytorch Variable, with shape [m, d]\n",
    "      y: pytorch Variable, with shape [n, d]\n",
    "    Returns:\n",
    "      dist: pytorch Variable, with shape [m, n]\n",
    "    \"\"\"\n",
    "    m, n = x.size(0), y.size(0)\n",
    "    xx = torch.pow(x, 2).sum(1, keepdim=True).expand(m, n)\n",
    "    yy = torch.pow(y, 2).sum(1, keepdim=True).expand(n, m).t()\n",
    "    dist = xx + yy\n",
    "    dist = dist - 2 * torch.matmul(x, y.t())\n",
    "    # dist.addmm_(1, -2, x, y.t())\n",
    "    dist = dist.clamp(min=1e-6).sqrt()  # for numerical stability\n",
    "    return dist\n",
    "\n",
    "\n",
    "def cosine_dist(x, y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      x: pytorch Variable, with shape [m, d]\n",
    "      y: pytorch Variable, with shape [n, d]\n",
    "    Returns:\n",
    "      dist: pytorch Variable, with shape [m, n]\n",
    "    \"\"\"\n",
    "    m, n = x.size(0), y.size(0)\n",
    "    x_norm = torch.pow(x, 2).sum(1, keepdim=True).sqrt().expand(m, n)\n",
    "    y_norm = torch.pow(y, 2).sum(1, keepdim=True).sqrt().expand(n, m).t()\n",
    "    xy_intersection = torch.mm(x, y.t())\n",
    "    dist = xy_intersection/(x_norm * y_norm)\n",
    "    dist = (1. - dist) / 2\n",
    "    return dist\n",
    "\n",
    "\n",
    "def hard_example_mining(dist_mat, labels, return_inds=False):\n",
    "    \"\"\"For each anchor, find the hardest positive and negative sample.\n",
    "    Args:\n",
    "      dist_mat: pytorch Variable, pair wise distance between samples, shape [N, N]\n",
    "      labels: pytorch LongTensor, with shape [N]\n",
    "      return_inds: whether to return the indices. Save time if `False`(?)\n",
    "    Returns:\n",
    "      dist_ap: pytorch Variable, distance(anchor, positive); shape [N]\n",
    "      dist_an: pytorch Variable, distance(anchor, negative); shape [N]\n",
    "      p_inds: pytorch LongTensor, with shape [N];\n",
    "        indices of selected hard positive samples; 0 <= p_inds[i] <= N - 1\n",
    "      n_inds: pytorch LongTensor, with shape [N];\n",
    "        indices of selected hard negative samples; 0 <= n_inds[i] <= N - 1\n",
    "    NOTE: Only consider the case in which all labels have same num of samples,\n",
    "      thus we can cope with all anchors in parallel.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(dist_mat.size()) == 2\n",
    "    assert dist_mat.size(0) == dist_mat.size(1)\n",
    "    N = dist_mat.size(0)\n",
    "\n",
    "    # shape [N, N]\n",
    "    is_pos = labels.expand(N, N).eq(labels.expand(N, N).t())\n",
    "    is_neg = labels.expand(N, N).ne(labels.expand(N, N).t())\n",
    "\n",
    "    # `dist_ap` means distance(anchor, positive)\n",
    "    # both `dist_ap` and `relative_p_inds` with shape [N, 1]\n",
    "    dist_ap, relative_p_inds = torch.max(\n",
    "        dist_mat[is_pos].contiguous().view(N, -1), 1, keepdim=True)\n",
    "    # print(dist_mat[is_pos].shape)\n",
    "    # `dist_an` means distance(anchor, negative)\n",
    "    # both `dist_an` and `relative_n_inds` with shape [N, 1]\n",
    "    dist_an, relative_n_inds = torch.min(\n",
    "        dist_mat[is_neg].contiguous().view(N, -1), 1, keepdim=True)\n",
    "    # shape [N]\n",
    "    dist_ap = dist_ap.squeeze(1)\n",
    "    dist_an = dist_an.squeeze(1)\n",
    "\n",
    "    if return_inds:\n",
    "        # shape [N, N]\n",
    "        ind = (labels.new().resize_as_(labels)\n",
    "               .copy_(torch.arange(0, N).long())\n",
    "               .unsqueeze(0).expand(N, N))\n",
    "        # shape [N, 1]\n",
    "        p_inds = torch.gather(\n",
    "            ind[is_pos].contiguous().view(N, -1), 1, relative_p_inds.data)\n",
    "        n_inds = torch.gather(\n",
    "            ind[is_neg].contiguous().view(N, -1), 1, relative_n_inds.data)\n",
    "        # shape [N]\n",
    "        p_inds = p_inds.squeeze(1)\n",
    "        n_inds = n_inds.squeeze(1)\n",
    "        return dist_ap, dist_an, p_inds, n_inds\n",
    "\n",
    "    return dist_ap, dist_an\n",
    "\n",
    "\n",
    "class TripletLoss(object):\n",
    "    \"\"\"\n",
    "    Triplet loss using HARDER example mining,\n",
    "    modified based on original triplet loss using hard example mining\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=None, hard_factor=0.0):\n",
    "        self.margin = margin\n",
    "        self.hard_factor = hard_factor\n",
    "        if margin is not None:\n",
    "            self.ranking_loss = nn.MarginRankingLoss(margin=margin)\n",
    "        else:\n",
    "            self.ranking_loss = nn.SoftMarginLoss()\n",
    "\n",
    "    def __call__(self, global_feat, labels, normalize_feature=False):\n",
    "        if normalize_feature:\n",
    "            global_feat = normalize(global_feat, axis=-1)\n",
    "        dist_mat = euclidean_dist(global_feat, global_feat)\n",
    "        dist_ap, dist_an = hard_example_mining(dist_mat, labels)\n",
    "\n",
    "        dist_ap *= (1.0 + self.hard_factor)\n",
    "        dist_an *= (1.0 - self.hard_factor)\n",
    "\n",
    "        y = dist_an.new().resize_as_(dist_an).fill_(1)\n",
    "        if self.margin is not None:\n",
    "            loss = self.ranking_loss(dist_an, dist_ap, y)\n",
    "        else:\n",
    "            loss = self.ranking_loss(dist_an - dist_ap, y)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class Tripletloss(nn.Module):\n",
    "    \"\"\"Triplet loss with hard positive/negative mining.\n",
    "\n",
    "    Reference:\n",
    "    Hermans et al. In Defense of the Triplet Loss for Person Re-Identification. arXiv:1703.07737.\n",
    "\n",
    "    Code imported from https://github.com/Cysu/open-reid/blob/master/reid/loss/triplet.py.\n",
    "\n",
    "    Args:\n",
    "        margin (float): margin for triplet.\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=0.3,hard_factor=0.0):\n",
    "        super(Tripletloss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.ranking_loss = nn.MarginRankingLoss(margin=margin)   \n",
    "                                                                 \n",
    "        self.hard_factor = hard_factor\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: feature matrix with shape (batch_size, feat_dim)\n",
    "            targets: ground truth labels with shape (num_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        n = inputs.size(0)\n",
    "\n",
    "        inputs = normalize(inputs,axis=-1)\n",
    "\n",
    "        dist =euclidean_dist(inputs,inputs)\n",
    "        # For each anchor, find the hardest positive and negative\n",
    "        mask = targets.expand(n, n).eq(targets.expand(n, n).t())\n",
    "        dist_ap, dist_an = [], []\n",
    "        for i in range(n):\n",
    "            if i < n/2:\n",
    "                dist_ap.append(dist[i][int(n/2):n][mask[i][int(n/2):n]].max().unsqueeze(0))\n",
    "                dist_an.append(dist[i][int(n/2):n][(mask[i] == 0)[int(n/2):n]].min().unsqueeze(0))\n",
    "            else:\n",
    "                dist_ap.append(dist[i][0:int(n/2)][mask[i][0:int(n/2)]].max().unsqueeze(0))\n",
    "                dist_an.append(dist[i][0:int(n/2)][(mask[i] == 0)[0:int(n/2)]].min().unsqueeze(0))\n",
    "        dist_ap = torch.cat(dist_ap)\n",
    "        dist_an = torch.cat(dist_an)\n",
    "        dist_ap *= (1.0 + self.hard_factor)\n",
    "        dist_an *= (1.0 - self.hard_factor)\n",
    "        # Compute ranking hinge loss\n",
    "        y = torch.ones_like(dist_an)\n",
    "\n",
    "        loss = self.ranking_loss(dist_an, dist_ap, y)\n",
    "        return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126df1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
